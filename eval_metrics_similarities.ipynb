{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcase_evaluator import DCASEEvaluator\n",
    "from models.audiosep import AudioSep\n",
    "from models.one_peace_encoder import ONE_PEACE_Encoder\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from utils import parse_yaml, load_ss_model\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import scipy.io.wavfile as wf\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import spectrogram\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lass_synthetic_validation.csv')\n",
    "df = df.assign(sdr=None, sisdr=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_csv = 'lass_synthetic_validation.csv'\n",
    "# dict_eval = pd.read_csv(validation_csv).set_index('file_name').to_dict()['caption']\n",
    "output_dir = 'lass_evaluation_real_output'\n",
    "audio_dir = 'lass_validation'\n",
    "sampling_rate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SDR\n",
    "def calculate_sdr(ref, est):\n",
    "    ref = np.array(ref)\n",
    "    est = np.array(est)\n",
    "    # estimated signal onto reference projection\n",
    "    a = np.dot(ref, est) / np.dot(ref, ref)\n",
    "    proj = a * ref\n",
    "    # error between estimated and projected signals\n",
    "    error = est - proj\n",
    "    # compute sdr\n",
    "    sdr = 10 * np.log10(np.sum(proj**2) / np.sum(error**2))\n",
    "    return sdr\n",
    "\n",
    "# Calculate SI-SDR\n",
    "def calculate_si_sdr(ref, est):\n",
    "    ref = np.array(ref)\n",
    "    est = np.array(est)\n",
    "    # normalize signals to remove scale dependency\n",
    "    ref_energy = np.dot(ref, ref)\n",
    "    scaling_factor = np.dot(ref, est) / ref_energy\n",
    "    proj = scaling_factor * ref\n",
    "    # error between estimated and projected signals\n",
    "    error = est - proj\n",
    "    # compute si-sdr\n",
    "    si_sdr = 10 * np.log10(np.sum(proj**2) / np.sum(error**2))\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for 1st row\n",
    "source1 = '692211_12333864-hq.wav'\n",
    "noise1 = '701692_6014995-hq.wav'\n",
    "source1_lib, sr = librosa.load(f\"{audio_dir}/{source1}\", sr=sampling_rate)\n",
    "noise1_lib, sr = librosa.load(f\"{audio_dir}/{noise1}\", sr=sampling_rate)\n",
    "mixture1 = source1_lib + (noise1_lib * 2)\n",
    "calculate_sdr(source1_lib, mixture1), calculate_si_sdr(source1_lib, mixture1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    source, sr = librosa.load(f\"{audio_dir}/{row['source']}.wav\", sr=sampling_rate)\n",
    "    noise, _ = librosa.load(f\"{audio_dir}/{row[' noise']}.wav\", sr=sampling_rate)\n",
    "    mixture = source + noise\n",
    "    df.at[index, 'sdr'] = calculate_sdr(source, mixture)\n",
    "    df.at[index, 'sisdr'] = calculate_si_sdr(source, mixture)\n",
    "df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config_yaml = 'config/audiosep_onepeace.yaml'\n",
    "encoder_checkpoint_path = '/fs/nexus-scratch/vla/finetune_al_retrieval.pt'\n",
    "\n",
    "# NOTE: best checkpoint on validation set\n",
    "ssnet_checkpoint_path = '/fs/nexus-scratch/vla/checkpoints/train/audiosep_onepeace,devices=1/step=140000.ckpt'\n",
    "sampling_rate=1600\n",
    "configs = parse_yaml(config_yaml)\n",
    "\n",
    "# ONE_PEACE modelhub expects some paths to be relative to this dir\n",
    "os.chdir('ONE-PEACE/')\n",
    "# TODO:path in shared scratch dir for now..., move to class project dir whenever we get that\n",
    "query_encoder = ONE_PEACE_Encoder(pretrained_path=encoder_checkpoint_path)\n",
    "os.chdir('..')\n",
    "\n",
    "# put ONE-PEACE model in eval model (probably unecessary)\n",
    "query_encoder.model.model.eval()\n",
    "\n",
    "pl_model = load_ss_model(\n",
    "    configs=configs,\n",
    "    checkpoint_path=ssnet_checkpoint_path,\n",
    "    query_encoder=query_encoder\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gather = []\n",
    "with torch.no_grad():\n",
    "    # iterate through dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # source file name (.wav file)\n",
    "        filename = row['source']\n",
    "        file_path = os.path.join(audio_dir, f'{filename}.wav')\n",
    "        \n",
    "        # load audio from test set\n",
    "        input_path = os.path.join(audio_dir, filename)\n",
    "        source, fs = librosa.load(input_path, sr=sampling_rate, mono=True)\n",
    "\n",
    "        # compute text embedding with ONE-PEACE query encoder\n",
    "        conditions = pl_model.query_encoder.get_query_embed(\n",
    "                        modality='text',\n",
    "                        text=[dict_eval[filename]],\n",
    "                        device=device \n",
    "        )\n",
    "\n",
    "        input_dict = {\n",
    "                        \"mixture\": torch.Tensor(source)[None, None, :].to(device),\n",
    "                        \"condition\": conditions,\n",
    "                    } \n",
    "\n",
    "        # output audio\n",
    "        sep_segment = pl_model.ss_model(input_dict)[\"waveform\"]\n",
    "\n",
    "        # TODO: compute ONE-PEACE embedding on sep_segment and dot w/ conditions for comparison in embedding space\n",
    "        # sep_segment_embd = pl_model.query_encoder.model\n",
    "        # sep_segment: (batch_size=1, channels_num=1, segment_samples)\n",
    "        sep_segment = sep_segment.squeeze(0).squeeze(0).data.cpu().numpy()\n",
    "\n",
    "\n",
    "        # write out .wav file\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        # wf.write(output_path, sampling_rate, sep_segment)\n",
    "\n",
    "        similarities = dict(\n",
    "            filename = filename\n",
    "        )\n",
    "\n",
    "        # COMPUTE SIMILARITIES\n",
    "        src_audios, audio_padding_masks = pl_model.query_encoder.model.process_audio([input_path])\n",
    "        audio_features = pl_model.query_encoder.model.extract_audio_features(src_audios, audio_padding_masks)\n",
    "        input_similarity = conditions @ audio_features.T\n",
    "        similarities['input_similarity'] = input_similarity.squeeze(0).cpu().numpy()[0]\n",
    "        # print(f'Text Prompt - Mixed Audio Input Similarity: {input_similarity}')\n",
    "\n",
    "        src_audios, audio_padding_masks = pl_model.query_encoder.model.process_audio([output_path])\n",
    "        audio_features = pl_model.query_encoder.model.extract_audio_features(src_audios, audio_padding_masks)\n",
    "        output_similarity = conditions @ audio_features.T\n",
    "        similarities['output_similarity'] = output_similarity.squeeze(0).cpu().numpy()[0]\n",
    "        # print(f'Text Prompt - Seperated Audio Output Similarity: {output_similarity}')\n",
    "\n",
    "        gather.append(similarities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AudioSep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
